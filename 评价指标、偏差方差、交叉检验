一、评价指标 AuC，RoC，mAP，Recall，Precision，F1-score

TP预测为正，实际为正 – true positive
FN预测为负，实际为正 – false negative
FP预测为正，实际为负 – false positive
TN预测为负，实际为负 – true negative

有多少比例的次品被检测出来了，有多少比例的次品被模型认为是良品而蒙混过关了，这些比例代表模型的商业价值，对应指标为召回率（Recall）
R=TP/(TP+FN)
模型的精度（Precision）P=TP/(TP+FP)
我们计算模型的准确性时需要同时考虑正类和负类，这也就是多分类问题的常用指标Accuracy=(TP+TN)/(TP+FN+FP+TN)
两者矛盾
PR图，以P为纵轴，R为横轴。

F1度量：F1=2*P*R/(P+R),是基于查准率和查全率的调和平均定义的
1/F1=1/2*(1/R+1/P)

2.ROC  AUC
ROC曲线是真正率和假正率在不同的阀值下之间的图形表示关系。通常用作权衡模型的敏感度与模型对一个错误分类报警的概率,研究模型泛化能力的好坏

真正率TPR=TP/(TP+FN) 纵轴
假正率FPR=FP/(FP+TN) 横轴

AUC为曲线下面的面积，作为评估指标，AUC值越大，说明模型越好

AP与mAP
AP就是Average Precision。在上文中提到，二分类任务中的Precision与分类阈值[公式]的选择有关。因此，我们可以对所有可能的分类阈值计算Precision，然后再取平均，就得到了Average Precision（AP）。

mAP就是mean Average Precision。它是为多分类任务所设计，计算过程中将C分类任务看作是C个二分类任务，然后分别计算每个二分类任务的AP，最后在个任务上求得平均。


二、偏差和方差（bias-variance）
Error（误差）=bias+variance+不可避免的误差

偏差（bias）：偏差衡量了模型的预测值与实际值之间的偏离关系。通常在深度学习中，我们每一次训练迭代出来的新模型，都会拿训练数据进行预测，
偏差就反应在预测值与实际值匹配度上，比如通常在keras运行中看到的准确度为96%，则说明是低偏差；反之，如果准确度只有70%，则说明是高偏差。
方差（variance）：方差描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。
从数学角度看，可以理解为每个预测值与预测均值差的平方和的再求平均数。通常在深度学习训练中，初始阶段模型复杂度不高，为低方差；随着训练量加大，模型逐步拟合训练数据，
复杂度开始变高，此时方差会逐渐变高。

低偏差，低方差：这是训练的理想模型，此时蓝色点集基本落在靶心范围内，且数据离散程度小，基本在靶心范围内；
低偏差，高方差：这是深度学习面临的最大问题，过拟合了。也就是模型太贴合训练数据了，导致其泛化（或通用）能力差，若遇到测试集，则准确度下降的厉害；
高偏差，低方差：这往往是训练的初始阶段；
高偏差，高方差：这是训练最糟糕的情况，准确度差，数据的离散程度也差。

三、检验
1、假设检验的基本思想

假设检验的基本思想是小概率反证法思想。
小概率思想是指小概率事件(P<0．01或P<0．05)在一次试验中基本上不会发生。
反证法思想是先提出假设(检验假设Ho)，再用适当的统计方法确定假设成立的可能性大小，如可能性小，则认为假设不成立，若可能性大，则还不能认为假设不成立。
